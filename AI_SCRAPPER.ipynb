{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install requests beautifulsoup4 pandas python-dotenv openpyxl lxml jupyter ipykernel\n",
        "\n",
        "#%% vscode.cell [id=#VSC-0846c592] [language=python]\n",
        "import sys\n",
        "print(sys.executable)\n",
        "#%% vscode.cell [id=#VSC-0754bb81] [language=python]\n",
        "import pkg_resources\n",
        "installed_packages = pkg_resources.working_set\n",
        "installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n",
        "print(installed_packages_list)\n",
        "#%% vscode.cell [id=#VSC-f34c4cd0] [language=python]\n",
        "# Africa AI Intelligence Platform - Advanced Multi-API Version\n",
        "# ==============================================================\n",
        "# Production-grade data collection using multiple API sources and advanced scraping\n",
        "\n",
        "# REQUIRED API KEYS (Get free tiers from):\n",
        "# - Crunchbase: https://data.crunchbase.com/docs\n",
        "# - GitHub: https://github.com/settings/tokens\n",
        "# - NewsAPI: https://newsapi.org\n",
        "# - SerpAPI: https://serpapi.com (Google Search API)\n",
        "# - PredictHQ: https://www.predicthq.com\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "import os\n",
        "from urllib.parse import quote_plus\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - [%(levelname)s] - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"AFRICA_AI\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - ADD YOUR API KEYS TO .env FILE\n",
        "# ============================================================================\n",
        "\n",
        "class APIConfig:\n",
        "    \"\"\"Store all API credentials from environment variables\"\"\"\n",
        "\n",
        "    # Get API keys from .env file\n",
        "    CRUNCHBASE_API_KEY = os.getenv('CRUNCHBASE_API_KEY', 'YOUR_KEY_HERE')\n",
        "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', 'YOUR_KEY_HERE')\n",
        "    NEWSAPI_KEY = os.getenv('NEWSAPI_API_KEY', 'YOUR_KEY_HERE')\n",
        "    SERPAPI_KEY = os.getenv('SERPAPI_API_KEY', 'YOUR_KEY_HERE')\n",
        "    RAPIDAPI_KEY = os.getenv('RAPIDAPI_API_KEY', 'YOUR_KEY_HERE')\n",
        "\n",
        "    HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    }\n",
        "\n",
        "#%% vscode.cell [id=#VSC-e7fd785c] [language=python]\n",
        "# ============================================================================\n",
        "# PIPELINE 1: CRUNCHBASE API - AI STARTUPS & COMPANIES\n",
        "# ============================================================================\n",
        "\n",
        "class CrunchbaseAIPipeline:\n",
        "    \"\"\"\n",
        "    Crunchbase API for startup data\n",
        "    Free tier: 200 calls/day\n",
        "    Signup: https://data.crunchbase.com/docs\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.crunchbase.com/api/v4\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def search_ai_companies(self, country: str, limit: int = 50) -> List[Dict]:\n",
        "        \"\"\"Search for AI companies in African countries\"\"\"\n",
        "\n",
        "        endpoint = f\"{self.BASE_URL}/searches/organizations\"\n",
        "\n",
        "        # Search query for AI companies\n",
        "        payload = {\n",
        "            \"field_ids\": [\n",
        "                \"identifier\", \"name\", \"short_description\", \"location_identifiers\",\n",
        "                \"categories\", \"num_employees_enum\", \"founded_on\", \"website_url\"\n",
        "            ],\n",
        "            \"query\": [\n",
        "                {\n",
        "                    \"type\": \"predicate\",\n",
        "                    \"field_id\": \"location_identifiers\",\n",
        "                    \"operator_id\": \"includes\",\n",
        "                    \"values\": [country.lower().replace(' ', '-')]\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"predicate\",\n",
        "                    \"field_id\": \"categories\",\n",
        "                    \"operator_id\": \"includes\",\n",
        "                    \"values\": [\"artificial-intelligence\", \"machine-learning\", \"data-analytics\"]\n",
        "                }\n",
        "            ],\n",
        "            \"limit\": limit\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            'X-cb-user-key': self.api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                endpoint,\n",
        "                json=payload,\n",
        "                headers=headers,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                companies = []\n",
        "\n",
        "                for entity in data.get('entities', []):\n",
        "                    props = entity.get('properties', {})\n",
        "                    companies.append({\n",
        "                        'company_name': props.get('name'),\n",
        "                        'description': props.get('short_description'),\n",
        "                        'country': country,\n",
        "                        'website': props.get('website_url'),\n",
        "                        'founded_year': props.get('founded_on', {}).get('value'),\n",
        "                        'categories': ', '.join([c.get('value') for c in props.get('categories', [])]),\n",
        "                        'employees': props.get('num_employees_enum'),\n",
        "                        'source': 'Crunchbase API',\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    })\n",
        "\n",
        "                logger.info(f\"Crunchbase: Found {len(companies)} companies in {country}\")\n",
        "                return companies\n",
        "            else:\n",
        "                logger.warning(f\"Crunchbase API error: {response.status_code}\")\n",
        "                return []\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Crunchbase error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def run(self, countries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries\"\"\"\n",
        "        all_companies = []\n",
        "\n",
        "        for country in countries:\n",
        "            companies = self.search_ai_companies(country)\n",
        "            all_companies.extend(companies)\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "        return pd.DataFrame(all_companies)\n",
        "\n",
        "\n",
        "\n",
        "#%% vscode.cell [id=#VSC-12c71735] [language=python]\n",
        "# ============================================================================\n",
        "# PIPELINE 2: GITHUB API - AI PROJECTS & TOOLS\n",
        "# ============================================================================\n",
        "\n",
        "class GitHubAIPipeline:\n",
        "    \"\"\"\n",
        "    GitHub API for AI repositories and projects\n",
        "    Free tier: 5000 requests/hour with token\n",
        "    Get token: https://github.com/settings/tokens\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.github.com\"\n",
        "\n",
        "    def __init__(self, token: str):\n",
        "        self.token = token\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'Authorization': f'token {token}',\n",
        "            'Accept': 'application/vnd.github.v3+json'\n",
        "        })\n",
        "\n",
        "    def search_african_ai_repos(self, country: str, sector: str = None) -> List[Dict]:\n",
        "        \"\"\"Search GitHub for African AI repositories\"\"\"\n",
        "\n",
        "        # Build search query\n",
        "        search_terms = [\n",
        "            f\"{country} AI\",\n",
        "            f\"{country} machine learning\",\n",
        "            f\"africa {sector}\" if sector else \"africa AI\"\n",
        "        ]\n",
        "\n",
        "        repos = []\n",
        "\n",
        "        for term in search_terms:\n",
        "            endpoint = f\"{self.BASE_URL}/search/repositories\"\n",
        "            params = {\n",
        "                'q': f'{term} language:python stars:>5',\n",
        "                'sort': 'stars',\n",
        "                'per_page': 30\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(endpoint, params=params, timeout=20)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for repo in data.get('items', []):\n",
        "                        # Check if truly African/AI related\n",
        "                        description = (repo.get('description') or '').lower()\n",
        "                        readme_url = repo.get('url') + '/readme'\n",
        "\n",
        "                        repos.append({\n",
        "                            'project_name': repo.get('full_name'),\n",
        "                            'description': repo.get('description'),\n",
        "                            'stars': repo.get('stargazers_count'),\n",
        "                            'language': repo.get('language'),\n",
        "                            'url': repo.get('html_url'),\n",
        "                            'country': country,\n",
        "                            'sector': sector or 'General AI',\n",
        "                            'topics': ', '.join(repo.get('topics', [])),\n",
        "                            'last_updated': repo.get('updated_at'),\n",
        "                            'source': 'GitHub API',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "                time.sleep(2)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"GitHub API error: {e}\")\n",
        "\n",
        "        logger.info(f\"GitHub: Found {len(repos)} repositories for {country}\")\n",
        "        return repos\n",
        "\n",
        "    def run(self, countries: List[str], sectors: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries and sectors\"\"\"\n",
        "        all_repos = []\n",
        "\n",
        "        for country in countries:\n",
        "            for sector in sectors:\n",
        "                repos = self.search_african_ai_repos(country, sector)\n",
        "                all_repos.extend(repos)\n",
        "\n",
        "        df = pd.DataFrame(all_repos)\n",
        "        return df.drop_duplicates(subset=['project_name'])\n",
        "\n",
        "\n",
        "\n",
        "#%% vscode.cell [id=#VSC-8a86fc61] [language=python]\n",
        "# ============================================================================\n",
        "# PIPELINE 3: NEWS API - AI NEWS & ADOPTION\n",
        "# ============================================================================\n",
        "\n",
        "class NewsAIPipeline:\n",
        "    \"\"\"\n",
        "    NewsAPI for recent AI news and developments\n",
        "    Free tier: 100 requests/day\n",
        "    Signup: https://newsapi.org\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://newsapi.org/v2\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def search_ai_news(self, country: str, days_back: int = 30) -> List[Dict]:\n",
        "        \"\"\"Search for AI-related news from African countries\"\"\"\n",
        "\n",
        "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
        "\n",
        "        endpoint = f\"{self.BASE_URL}/everything\"\n",
        "\n",
        "        queries = [\n",
        "            f\"{country} artificial intelligence\",\n",
        "            f\"{country} AI startup\",\n",
        "            f\"{country} machine learning technology\"\n",
        "        ]\n",
        "\n",
        "        articles = []\n",
        "\n",
        "        for query in queries:\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'from': from_date,\n",
        "                'language': 'en',\n",
        "                'sortBy': 'relevancy',\n",
        "                'apiKey': self.api_key,\n",
        "                'pageSize': 20\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(endpoint, params=params, timeout=20)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for article in data.get('articles', []):\n",
        "                        articles.append({\n",
        "                            'title': article.get('title'),\n",
        "                            'description': article.get('description'),\n",
        "                            'country': country,\n",
        "                            'source': article.get('source', {}).get('name'),\n",
        "                            'url': article.get('url'),\n",
        "                            'published_at': article.get('publishedAt'),\n",
        "                            'content_preview': article.get('content', '')[:300],\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"NewsAPI error: {e}\")\n",
        "\n",
        "        logger.info(f\"NewsAPI: Found {len(articles)} articles for {country}\")\n",
        "        return articles\n",
        "\n",
        "    def run(self, countries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries\"\"\"\n",
        "        all_articles = []\n",
        "\n",
        "        for country in countries:\n",
        "            articles = self.search_ai_news(country)\n",
        "            all_articles.extend(articles)\n",
        "\n",
        "        df = pd.DataFrame(all_articles)\n",
        "        return df.drop_duplicates(subset=['title', 'url'])\n",
        "\n",
        "\n",
        "\n",
        "#%% vscode.cell [id=#VSC-96d7b6b1] [language=python]\n",
        "# ============================================================================\n",
        "# PIPELINE 4: SERP API - GOOGLE SEARCH FOR GOVERNMENT & POLICY\n",
        "# ============================================================================\n",
        "\n",
        "class SerpAPIGovPipeline:\n",
        "    \"\"\"\n",
        "    SerpAPI for Google searches (government AI policies)\n",
        "    Free tier: 100 searches/month\n",
        "    Signup: https://serpapi.com\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://serpapi.com/search\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def search_government_ai(self, country: str) -> List[Dict]:\n",
        "        \"\"\"Search for government AI initiatives and policies\"\"\"\n",
        "\n",
        "        queries = [\n",
        "            f\"{country} government artificial intelligence strategy\",\n",
        "            f\"{country} national AI policy\",\n",
        "            f\"{country} public sector AI implementation\",\n",
        "            f\"{country} AI governance framework\"\n",
        "        ]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for query in queries:\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'api_key': self.api_key,\n",
        "                'num': 10,\n",
        "                'hl': 'en'\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(self.BASE_URL, params=params, timeout=20)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for item in data.get('organic_results', []):\n",
        "                        results.append({\n",
        "                            'title': item.get('title'),\n",
        "                            'snippet': item.get('snippet'),\n",
        "                            'url': item.get('link'),\n",
        "                            'country': country,\n",
        "                            'query_type': 'Government AI Policy',\n",
        "                            'position': item.get('position'),\n",
        "                            'source': 'Google Search (SerpAPI)',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"SerpAPI error: {e}\")\n",
        "\n",
        "        logger.info(f\"SerpAPI: Found {len(results)} policy documents for {country}\")\n",
        "        return results\n",
        "\n",
        "    def run(self, countries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries\"\"\"\n",
        "        all_results = []\n",
        "\n",
        "        for country in countries:\n",
        "            results = self.search_government_ai(country)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        return pd.DataFrame(all_results)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PIPELINE 5: ADVANCED WEB SCRAPING - AFRICAN TECH SITES\n",
        "# ============================================================================\n",
        "\n",
        "class AfricanTechScraper:\n",
        "    \"\"\"\n",
        "    Targeted scraping of African tech news sites\n",
        "    No API required - direct scraping with proper parsing\n",
        "    \"\"\"\n",
        "\n",
        "    SOURCES = {\n",
        "        'Disrupt Africa': 'https://disrupt-africa.com/category/fintech/',\n",
        "        'TechCabal': 'https://techcabal.com/category/startups/',\n",
        "        'African Tech Roundup': 'https://africantechroundup.com',\n",
        "        'Ventureburn': 'https://ventureburn.com/category/tech/'\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(APIConfig.HEADERS)\n",
        "\n",
        "    def scrape_disrupt_africa(self) -> List[Dict]:\n",
        "        \"\"\"Scrape Disrupt Africa for AI startups\"\"\"\n",
        "\n",
        "        companies = []\n",
        "        url = 'https://disrupt-africa.com'\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find article blocks\n",
        "            articles = soup.find_all('article', class_='post')\n",
        "\n",
        "            for article in articles[:20]:\n",
        "                title_tag = article.find('h2', class_='entry-title')\n",
        "                if title_tag:\n",
        "                    title = title_tag.get_text(strip=True)\n",
        "                    link = title_tag.find('a')['href'] if title_tag.find('a') else None\n",
        "\n",
        "                    excerpt = article.find('div', class_='entry-excerpt')\n",
        "                    description = excerpt.get_text(strip=True) if excerpt else ''\n",
        "\n",
        "                    # Check if AI-related\n",
        "                    if any(kw in (title + description).lower() for kw in ['ai', 'artificial', 'machine learning', 'ml']):\n",
        "                        companies.append({\n",
        "                            'company_name': title,\n",
        "                            'description': description[:500],\n",
        "                            'url': link,\n",
        "                            'source': 'Disrupt Africa',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "            logger.info(f\"Disrupt Africa: Scraped {len(companies)} AI companies\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Disrupt Africa scraping error: {e}\")\n",
        "\n",
        "        return companies\n",
        "\n",
        "    def scrape_techcabal(self) -> List[Dict]:\n",
        "        \"\"\"Scrape TechCabal for AI news\"\"\"\n",
        "\n",
        "        articles = []\n",
        "        url = 'https://techcabal.com'\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find recent articles\n",
        "            post_links = soup.find_all('h2', class_='post-title')\n",
        "\n",
        "            for post in post_links[:15]:\n",
        "                link_tag = post.find('a')\n",
        "                if link_tag:\n",
        "                    title = link_tag.get_text(strip=True)\n",
        "                    link = link_tag['href']\n",
        "\n",
        "                    if any(kw in title.lower() for kw in ['ai', 'artificial', 'tech', 'startup']):\n",
        "                        articles.append({\n",
        "                            'title': title,\n",
        "                            'url': link,\n",
        "                            'source': 'TechCabal',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "            logger.info(f\"TechCabal: Scraped {len(articles)} articles\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"TechCabal scraping error: {e}\")\n",
        "\n",
        "        return articles\n",
        "\n",
        "    def run(self) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Run all scrapers\"\"\"\n",
        "\n",
        "        companies = self.scrape_disrupt_africa()\n",
        "        articles = self.scrape_techcabal()\n",
        "\n",
        "        return {\n",
        "            'companies': pd.DataFrame(companies),\n",
        "            'articles': pd.DataFrame(articles)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "#%% vscode.cell [id=#VSC-fdec63d4] [language=python]\n",
        "# ============================================================================\n",
        "# MASTER ORCHESTRATOR\n",
        "# ============================================================================\n",
        "\n",
        "class AfricaAIMasterPipeline:\n",
        "    \"\"\"Orchestrate all data collection pipelines\"\"\"\n",
        "\n",
        "    def __init__(self, config: APIConfig):\n",
        "        self.config = config\n",
        "        self.results = {}\n",
        "\n",
        "    def run_all_pipelines(self, countries: List[str], sectors: List[str]):\n",
        "        \"\"\"Execute all data collection pipelines\"\"\"\n",
        "\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"STARTING AFRICA AI INTELLIGENCE PLATFORM\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "        # # Pipeline 1: Crunchbase Companies\n",
        "        # if self.config.CRUNCHBASE_API_KEY != 'YOUR_KEY_HERE':\n",
        "        #     logger.info(\"\\n[1/5] Running Crunchbase Pipeline...\")\n",
        "        #     crunchbase = CrunchbaseAIPipeline(self.config.CRUNCHBASE_API_KEY)\n",
        "        #     self.results['crunchbase_companies'] = crunchbase.run(countries)\n",
        "        #     if 'crunchbase_companies' in self.results and not self.results['crunchbase_companies'].empty:\n",
        "        #         logger.info(f\"‚úÖ Crunchbase Pipeline finished: Collected {len(self.results['crunchbase_companies'])} companies.\")\n",
        "        #     else:\n",
        "        #         logger.warning(\"üö´ Crunchbase Pipeline finished: No companies collected or an issue occurred.\")\n",
        "        # else:\n",
        "        #     logger.warning(\"[1/5] Skipping Crunchbase - No API key\")\n",
        "\n",
        "        # Pipeline 2: GitHub Projects\n",
        "        if self.config.GITHUB_TOKEN != 'YOUR_KEY_HERE':\n",
        "            logger.info(\"\\n[2/5] Running GitHub Pipeline...\")\n",
        "            github = GitHubAIPipeline(self.config.GITHUB_TOKEN)\n",
        "            self.results['github_projects'] = github.run(countries, sectors)\n",
        "            if 'github_projects' in self.results and not self.results['github_projects'].empty:\n",
        "                logger.info(f\"‚úÖ GitHub Pipeline finished: Collected {len(self.results['github_projects'])} projects.\")\n",
        "            else:\n",
        "                logger.warning(\"üö´ GitHub Pipeline finished: No projects collected or an issue occurred.\")\n",
        "        else:\n",
        "            logger.warning(\"[2/5] Skipping GitHub - No API token\")\n",
        "\n",
        "        # Pipeline 3: News Articles\n",
        "        if self.config.NEWSAPI_KEY != 'YOUR_KEY_HERE':\n",
        "            logger.info(\"\\n[3/5] Running NewsAPI Pipeline...\")\n",
        "            news = NewsAIPipeline(self.config.NEWSAPI_KEY)\n",
        "            self.results['news_articles'] = news.run(countries)\n",
        "            if 'news_articles' in self.results and not self.results['news_articles'].empty:\n",
        "                logger.info(f\"‚úÖ NewsAPI Pipeline finished: Collected {len(self.results['news_articles'])} articles.\")\n",
        "            else:\n",
        "                logger.warning(\"üö´ NewsAPI Pipeline finished: No articles collected or an issue occurred.\")\n",
        "        else:\n",
        "            logger.warning(\"[3/5] Skipping NewsAPI - No API key\")\n",
        "\n",
        "        # Pipeline 4: Government Policies\n",
        "        if self.config.SERPAPI_KEY != 'YOUR_KEY_HERE':\n",
        "            logger.info(\"\\n[4/5] Running SerpAPI Government Pipeline...\")\n",
        "            serp = SerpAPIGovPipeline(self.config.SERPAPI_KEY)\n",
        "            self.results['government_policies'] = serp.run(countries)\n",
        "            if 'government_policies' in self.results and not self.results['government_policies'].empty:\n",
        "                logger.info(f\"‚úÖ SerpAPI Pipeline finished: Collected {len(self.results['government_policies'])} policy documents.\")\n",
        "            else:\n",
        "                logger.warning(\"üö´ SerpAPI Pipeline finished: No policy documents collected or an issue occurred.\")\n",
        "        else:\n",
        "            logger.warning(\"[4/5] Skipping SerpAPI - No API key\")\n",
        "\n",
        "        # Pipeline 5: African Tech Sites (No API needed)\n",
        "        logger.info(\"\\n[5/5] Running African Tech Scraper...\")\n",
        "        tech_scraper = AfricanTechScraper()\n",
        "        scraper_results = tech_scraper.run()\n",
        "        self.results.update(scraper_results)\n",
        "        if 'companies' in self.results and not self.results['companies'].empty:\n",
        "            logger.info(f\"‚úÖ African Tech Scraper (Companies) finished: Collected {len(self.results['companies'])} companies.\")\n",
        "        else:\n",
        "            logger.warning(\"üö´ African Tech Scraper (Companies) finished: No companies collected or an issue occurred.\")\n",
        "        if 'articles' in self.results and not self.results['articles'].empty:\n",
        "            logger.info(f\"‚úÖ African Tech Scraper (Articles) finished: Collected {len(self.results['articles'])} articles.\")\n",
        "        else:\n",
        "            logger.warning(\"üö´ African Tech Scraper (Articles) finished: No articles collected or an issue occurred.\")\n",
        "\n",
        "        logger.info(\"\\n\" + \"=\"*80)\n",
        "        logger.info(\"ALL PIPELINES COMPLETED\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def generate_summary(self):\n",
        "        \"\"\"Generate summary statistics\"\"\"\n",
        "        summary = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_datasets': len(self.results),\n",
        "            'datasets': {}\n",
        "        }\n",
        "\n",
        "        for name, df in self.results.items():\n",
        "            if isinstance(df, pd.DataFrame):\n",
        "                summary['datasets'][name] = {\n",
        "                    'records': len(df),\n",
        "                    'columns': list(df.columns)\n",
        "                }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def export_results(self, output_dir: str = './'):\n",
        "        \"\"\"Export all results to files\"\"\"\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Check if there's any non-empty DataFrame to export\n",
        "        has_data = any(isinstance(df, pd.DataFrame) and not df.empty for df in self.results.values())\n",
        "\n",
        "        if not has_data:\n",
        "            logger.warning(\"No data collected to export. Skipping file export.\")\n",
        "            return\n",
        "\n",
        "        # Export individual CSVs\n",
        "        for name, df in self.results.items():\n",
        "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                filename = f\"{output_dir}africa_ai_{name}_{timestamp}.csv\"\n",
        "                df.to_csv(filename, index=False)\n",
        "                logger.info(f\"‚úÖ Exported: {filename} ({len(df)} records)\")\n",
        "\n",
        "        # Export combined Excel\n",
        "        excel_file = f\"{output_dir}africa_ai_complete_{timestamp}.xlsx\"\n",
        "        try:\n",
        "            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "                for name, df in self.results.items():\n",
        "                    if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                        df.to_excel(writer, sheet_name=name[:31], index=False)\n",
        "            logger.info(f\"‚úÖ Combined Excel: {excel_file}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error exporting combined Excel file: {e}\")\n",
        "\n",
        "        # Export summary\n",
        "        summary = self.generate_summary()\n",
        "        summary_file = f\"{output_dir}pipeline_summary_{timestamp}.json\"\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        logger.info(f\"‚úÖ Summary: {summary_file}\")\n",
        "\n",
        "#%% vscode.cell [id=#VSC-4e7bb9a1] [language=python]\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Configuration\n",
        "    COUNTRIES = [\n",
        "        'Algeria', 'Angola', 'Benin', 'Botswana', 'Burkina Faso', 'Burundi',\n",
        "        'Cabo Verde', 'Cameroon', 'Central African Republic', 'Chad', 'Comoros',\n",
        "        'Congo, Dem. Rep.', 'Congo, Rep.', 'Cote dIvoire', 'Djibouti', 'Egypt',\n",
        "        'Equatorial Guinea', 'Eritrea', 'Eswatini', 'Ethiopia', 'Gabon', 'Gambia',\n",
        "        'Ghana', 'Guinea', 'Guinea-Bissau', 'Kenya', 'Lesotho', 'Liberia', 'Libya',\n",
        "        'Madagascar', 'Malawi', 'Mali', 'Mauritania', 'Mauritius', 'Morocco',\n",
        "        'Mozambique', 'Namibia', 'Niger', 'Nigeria', 'Rwanda', 'Sao Tome and Principe',\n",
        "        'Senegal', 'Seychelles', 'Sierra Leone', 'Somalia', 'South Africa', 'South Sudan',\n",
        "        'Sudan', 'Tanzania', 'Togo', 'Tunisia', 'Uganda', 'Zambia', 'Zimbabwe'\n",
        "    ]\n",
        "\n",
        "    SECTORS = [\n",
        "        'Agriculture', 'Healthcare', 'Fintech', 'Education',\n",
        "        'Logistics', 'Energy', 'Governance', 'Telecommunications', 'Mining',\n",
        "        'Tourism', 'Manufacturing', 'Retail', 'Media', 'Real Estate',\n",
        "        'Transportation', 'Financial Services', 'Public Sector', 'Utilities',\n",
        "        'Environment', 'Smart Cities', 'Security', 'Creative Industry','Insurance'\n",
        "    ]\n",
        "\n",
        "    # Initialize pipeline\n",
        "    config = APIConfig()\n",
        "    pipeline = AfricaAIMasterPipeline(config)\n",
        "\n",
        "    # Run all pipelines\n",
        "    results = pipeline.run_all_pipelines(COUNTRIES, SECTORS)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESULTS PREVIEW\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    for name, df in results.items():\n",
        "        if isinstance(df, pd.DataFrame):\n",
        "            print(f\"\\nüìä {name.upper()}: {len(df)} records\")\n",
        "            print(\"-\" * 60)\n",
        "            if not df.empty:\n",
        "                print(df.head(3))\n",
        "\n",
        "    # Export everything\n",
        "    pipeline.export_results()\n",
        "\n",
        "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
        "    print(\"\\nüìù NEXT STEPS:\")\n",
        "    print(\"1. Add your API keys to APIConfig class\")\n",
        "    print(\"2. Run the pipelines\")\n",
        "    print(\"3. Check exported CSV and Excel files\")\n",
        "    print(\"4. Analyze the data for insights\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python3.13\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['asttokens==3.0.1', 'autocommand==2.2.2', 'backports.tarfile==1.2.0', 'comm==0.2.3', 'debugpy==1.8.19', 'decorator==5.2.1', 'executing==2.2.1', 'importlib-metadata==8.0.0', 'inflect==7.3.1', 'ipykernel==7.1.0', 'ipython-pygments-lexers==1.1.1', 'ipython==9.9.0', 'jaraco.collections==5.1.0', 'jaraco.context==5.3.0', 'jaraco.functools==4.0.1', 'jaraco.text==3.12.1', 'jedi==0.19.2', 'jupyter-client==8.8.0', 'jupyter-core==5.9.1', 'matplotlib-inline==0.2.1', 'more-itertools==10.3.0', 'nest-asyncio==1.6.0', 'packaging==25.0', 'parso==0.8.5', 'pexpect==4.9.0', 'pip==25.3', 'platformdirs==4.5.1', 'prompt-toolkit==3.0.52', 'psutil==7.2.1', 'ptyprocess==0.7.0', 'pure-eval==0.2.3', 'pygments==2.19.2', 'python-dateutil==2.9.0.post0', 'pyzmq==27.1.0', 'setuptools==80.9.0', 'six==1.17.0', 'stack-data==0.6.3', 'tomli==2.0.1', 'tornado==6.5.4', 'traitlets==5.14.3', 'typeguard==4.3.0', 'typing-extensions==4.12.2', 'wcwidth==0.2.14', 'wheel==0.45.1', 'zipp==3.19.2']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20767/3830654686.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "installed_packages = pkg_resources.working_set\n",
        "installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n",
        "print(installed_packages_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "20etx2pt7lXx"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'requests'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Africa AI Intelligence Platform - Advanced Multi-API Version\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ==============================================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Production-grade data collection using multiple API sources and advanced scraping\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# - SerpAPI: https://serpapi.com (Google Search API)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# - PredictHQ: https://www.predicthq.com\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
          ]
        }
      ],
      "source": [
        "# Africa AI Intelligence Platform - Advanced Multi-API Version\n",
        "# ==============================================================\n",
        "# Production-grade data collection using multiple API sources and advanced scraping\n",
        "\n",
        "# REQUIRED API KEYS (Get free tiers from):\n",
        "# - Crunchbase: https://data.crunchbase.com/docs\n",
        "# - GitHub: https://github.com/settings/tokens\n",
        "# - NewsAPI: https://newsapi.org\n",
        "# - SerpAPI: https://serpapi.com (Google Search API)\n",
        "# - PredictHQ: https://www.predicthq.com\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "import os\n",
        "from urllib.parse import quote_plus\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - [%(levelname)s] - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"AFRICA_AI\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - ADD YOUR API KEYS TO .env FILE\n",
        "# ============================================================================\n",
        "\n",
        "class APIConfig:\n",
        "    \"\"\"Store all API credentials from environment variables\"\"\"\n",
        "\n",
        "    # Get API keys from .env file\n",
        "    CRUNCHBASE_API_KEY = os.getenv('CRUNCHBASE_API_KEY', 'YOUR_KEY_HERE')\n",
        "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', 'YOUR_KEY_HERE')\n",
        "    NEWSAPI_KEY = os.getenv('NEWSAPI_API_KEY', 'YOUR_KEY_HERE')\n",
        "    SERPAPI_KEY = os.getenv('SERPAPI_API_KEY', 'YOUR_KEY_HERE')\n",
        "    RAPIDAPI_KEY = os.getenv('RAPIDAPI_API_KEY', 'YOUR_KEY_HERE')\n",
        "\n",
        "    HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF7-CGX57z98"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PIPELINE 1: CRUNCHBASE API - AI STARTUPS & COMPANIES\n",
        "# ============================================================================\n",
        "\n",
        "class CrunchbaseAIPipeline:\n",
        "    \"\"\"\n",
        "    Crunchbase API for startup data\n",
        "    Free tier: 200 calls/day\n",
        "    Signup: https://data.crunchbase.com/docs\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.crunchbase.com/api/v4\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def search_ai_companies(self, country: str, limit: int = 50) -> List[Dict]:\n",
        "        \"\"\"Search for AI companies in African countries\"\"\"\n",
        "\n",
        "        endpoint = f\"{self.BASE_URL}/searches/organizations\"\n",
        "\n",
        "        # Search query for AI companies\n",
        "        payload = {\n",
        "            \"field_ids\": [\n",
        "                \"identifier\", \"name\", \"short_description\", \"location_identifiers\",\n",
        "                \"categories\", \"num_employees_enum\", \"founded_on\", \"website_url\"\n",
        "            ],\n",
        "            \"query\": [\n",
        "                {\n",
        "                    \"type\": \"predicate\",\n",
        "                    \"field_id\": \"location_identifiers\",\n",
        "                    \"operator_id\": \"includes\",\n",
        "                    \"values\": [country.lower().replace(' ', '-')]\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"predicate\",\n",
        "                    \"field_id\": \"categories\",\n",
        "                    \"operator_id\": \"includes\",\n",
        "                    \"values\": [\"artificial-intelligence\", \"machine-learning\", \"data-analytics\"]\n",
        "                }\n",
        "            ],\n",
        "            \"limit\": limit\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            'X-cb-user-key': self.api_key,\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.post(\n",
        "                endpoint,\n",
        "                json=payload,\n",
        "                headers=headers,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                companies = []\n",
        "\n",
        "                for entity in data.get('entities', []):\n",
        "                    props = entity.get('properties', {})\n",
        "                    companies.append({\n",
        "                        'company_name': props.get('name'),\n",
        "                        'description': props.get('short_description'),\n",
        "                        'country': country,\n",
        "                        'website': props.get('website_url'),\n",
        "                        'founded_year': props.get('founded_on', {}).get('value'),\n",
        "                        'categories': ', '.join([c.get('value') for c in props.get('categories', [])]),\n",
        "                        'employees': props.get('num_employees_enum'),\n",
        "                        'source': 'Crunchbase API',\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    })\n",
        "\n",
        "                logger.info(f\"Crunchbase: Found {len(companies)} companies in {country}\")\n",
        "                return companies\n",
        "            else:\n",
        "                logger.warning(f\"Crunchbase API error: {response.status_code}\")\n",
        "                return []\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Crunchbase error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def run(self, countries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries\"\"\"\n",
        "        all_companies = []\n",
        "\n",
        "        for country in countries:\n",
        "            companies = self.search_ai_companies(country)\n",
        "            all_companies.extend(companies)\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "        return pd.DataFrame(all_companies)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GGRYGmR8DE9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PIPELINE 2: GITHUB API - AI PROJECTS & TOOLS\n",
        "# ============================================================================\n",
        "\n",
        "class GitHubAIPipeline:\n",
        "    \"\"\"\n",
        "    GitHub API for AI repositories and projects\n",
        "    Free tier: 5000 requests/hour with token\n",
        "    Get token: https://github.com/settings/tokens\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.github.com\"\n",
        "\n",
        "    def __init__(self, token: str):\n",
        "        self.token = token\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'Authorization': f'token {token}',\n",
        "            'Accept': 'application/vnd.github.v3+json'\n",
        "        })\n",
        "\n",
        "    def search_african_ai_repos(self, country: str, sector: str = None) -> List[Dict]:\n",
        "        \"\"\"Search GitHub for African AI repositories\"\"\"\n",
        "\n",
        "        # Build search query\n",
        "        search_terms = [\n",
        "            f\"{country} AI\",\n",
        "            f\"{country} machine learning\",\n",
        "            f\"africa {sector}\" if sector else \"africa AI\"\n",
        "        ]\n",
        "\n",
        "        repos = []\n",
        "\n",
        "        for term in search_terms:\n",
        "            endpoint = f\"{self.BASE_URL}/search/repositories\"\n",
        "            params = {\n",
        "                'q': f'{term} language:python stars:>5',\n",
        "                'sort': 'stars',\n",
        "                'per_page': 30\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(endpoint, params=params, timeout=20)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for repo in data.get('items', []):\n",
        "                        # Check if truly African/AI related\n",
        "                        description = (repo.get('description') or '').lower()\n",
        "                        readme_url = repo.get('url') + '/readme'\n",
        "\n",
        "                        repos.append({\n",
        "                            'project_name': repo.get('full_name'),\n",
        "                            'description': repo.get('description'),\n",
        "                            'stars': repo.get('stargazers_count'),\n",
        "                            'language': repo.get('language'),\n",
        "                            'url': repo.get('html_url'),\n",
        "                            'country': country,\n",
        "                            'sector': sector or 'General AI',\n",
        "                            'topics': ', '.join(repo.get('topics', [])),\n",
        "                            'last_updated': repo.get('updated_at'),\n",
        "                            'source': 'GitHub API',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "                time.sleep(2)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"GitHub API error: {e}\")\n",
        "\n",
        "        logger.info(f\"GitHub: Found {len(repos)} repositories for {country}\")\n",
        "        return repos\n",
        "\n",
        "    def run(self, countries: List[str], sectors: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries and sectors\"\"\"\n",
        "        all_repos = []\n",
        "\n",
        "        for country in countries:\n",
        "            for sector in sectors:\n",
        "                repos = self.search_african_ai_repos(country, sector)\n",
        "                all_repos.extend(repos)\n",
        "\n",
        "        df = pd.DataFrame(all_repos)\n",
        "        return df.drop_duplicates(subset=['project_name'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRUWxidM8Klx"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PIPELINE 3: NEWS API - AI NEWS & ADOPTION\n",
        "# ============================================================================\n",
        "\n",
        "class NewsAIPipeline:\n",
        "    \"\"\"\n",
        "    NewsAPI for recent AI news and developments\n",
        "    Free tier: 100 requests/day\n",
        "    Signup: https://newsapi.org\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://newsapi.org/v2\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def search_ai_news(self, country: str, days_back: int = 30) -> List[Dict]:\n",
        "        \"\"\"Search for AI-related news from African countries\"\"\"\n",
        "\n",
        "        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
        "\n",
        "        endpoint = f\"{self.BASE_URL}/everything\"\n",
        "\n",
        "        queries = [\n",
        "            f\"{country} artificial intelligence\",\n",
        "            f\"{country} AI startup\",\n",
        "            f\"{country} machine learning technology\"\n",
        "        ]\n",
        "\n",
        "        articles = []\n",
        "\n",
        "        for query in queries:\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'from': from_date,\n",
        "                'language': 'en',\n",
        "                'sortBy': 'relevancy',\n",
        "                'apiKey': self.api_key,\n",
        "                'pageSize': 20\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(endpoint, params=params, timeout=20)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for article in data.get('articles', []):\n",
        "                        articles.append({\n",
        "                            'title': article.get('title'),\n",
        "                            'description': article.get('description'),\n",
        "                            'country': country,\n",
        "                            'source': article.get('source', {}).get('name'),\n",
        "                            'url': article.get('url'),\n",
        "                            'published_at': article.get('publishedAt'),\n",
        "                            'content_preview': article.get('content', '')[:300],\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"NewsAPI error: {e}\")\n",
        "\n",
        "        logger.info(f\"NewsAPI: Found {len(articles)} articles for {country}\")\n",
        "        return articles\n",
        "\n",
        "    def run(self, countries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries\"\"\"\n",
        "        all_articles = []\n",
        "\n",
        "        for country in countries:\n",
        "            articles = self.search_ai_news(country)\n",
        "            all_articles.extend(articles)\n",
        "\n",
        "        df = pd.DataFrame(all_articles)\n",
        "        return df.drop_duplicates(subset=['title', 'url'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et-dAUOB8Sdm"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PIPELINE 4: SERP API - GOOGLE SEARCH FOR GOVERNMENT & POLICY\n",
        "# ============================================================================\n",
        "\n",
        "class SerpAPIGovPipeline:\n",
        "    \"\"\"\n",
        "    SerpAPI for Google searches (government AI policies)\n",
        "    Free tier: 100 searches/month\n",
        "    Signup: https://serpapi.com\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL = \"https://serpapi.com/search\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def search_government_ai(self, country: str) -> List[Dict]:\n",
        "        \"\"\"Search for government AI initiatives and policies\"\"\"\n",
        "\n",
        "        queries = [\n",
        "            f\"{country} government artificial intelligence strategy\",\n",
        "            f\"{country} national AI policy\",\n",
        "            f\"{country} public sector AI implementation\",\n",
        "            f\"{country} AI governance framework\"\n",
        "        ]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for query in queries:\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'api_key': self.api_key,\n",
        "                'num': 10,\n",
        "                'hl': 'en'\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(self.BASE_URL, params=params, timeout=20)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    for item in data.get('organic_results', []):\n",
        "                        results.append({\n",
        "                            'title': item.get('title'),\n",
        "                            'snippet': item.get('snippet'),\n",
        "                            'url': item.get('link'),\n",
        "                            'country': country,\n",
        "                            'query_type': 'Government AI Policy',\n",
        "                            'position': item.get('position'),\n",
        "                            'source': 'Google Search (SerpAPI)',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"SerpAPI error: {e}\")\n",
        "\n",
        "        logger.info(f\"SerpAPI: Found {len(results)} policy documents for {country}\")\n",
        "        return results\n",
        "\n",
        "    def run(self, countries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run for multiple countries\"\"\"\n",
        "        all_results = []\n",
        "\n",
        "        for country in countries:\n",
        "            results = self.search_government_ai(country)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        return pd.DataFrame(all_results)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PIPELINE 5: ADVANCED WEB SCRAPING - AFRICAN TECH SITES\n",
        "# ============================================================================\n",
        "\n",
        "class AfricanTechScraper:\n",
        "    \"\"\"\n",
        "    Targeted scraping of African tech news sites\n",
        "    No API required - direct scraping with proper parsing\n",
        "    \"\"\"\n",
        "\n",
        "    SOURCES = {\n",
        "        'Disrupt Africa': 'https://disrupt-africa.com/category/fintech/',\n",
        "        'TechCabal': 'https://techcabal.com/category/startups/',\n",
        "        'African Tech Roundup': 'https://africantechroundup.com',\n",
        "        'Ventureburn': 'https://ventureburn.com/category/tech/'\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(APIConfig.HEADERS)\n",
        "\n",
        "    def scrape_disrupt_africa(self) -> List[Dict]:\n",
        "        \"\"\"Scrape Disrupt Africa for AI startups\"\"\"\n",
        "\n",
        "        companies = []\n",
        "        url = 'https://disrupt-africa.com'\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find article blocks\n",
        "            articles = soup.find_all('article', class_='post')\n",
        "\n",
        "            for article in articles[:20]:\n",
        "                title_tag = article.find('h2', class_='entry-title')\n",
        "                if title_tag:\n",
        "                    title = title_tag.get_text(strip=True)\n",
        "                    link = title_tag.find('a')['href'] if title_tag.find('a') else None\n",
        "\n",
        "                    excerpt = article.find('div', class_='entry-excerpt')\n",
        "                    description = excerpt.get_text(strip=True) if excerpt else ''\n",
        "\n",
        "                    # Check if AI-related\n",
        "                    if any(kw in (title + description).lower() for kw in ['ai', 'artificial', 'machine learning', 'ml']):\n",
        "                        companies.append({\n",
        "                            'company_name': title,\n",
        "                            'description': description[:500],\n",
        "                            'url': link,\n",
        "                            'source': 'Disrupt Africa',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "            logger.info(f\"Disrupt Africa: Scraped {len(companies)} AI companies\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Disrupt Africa scraping error: {e}\")\n",
        "\n",
        "        return companies\n",
        "\n",
        "    def scrape_techcabal(self) -> List[Dict]:\n",
        "        \"\"\"Scrape TechCabal for AI news\"\"\"\n",
        "\n",
        "        articles = []\n",
        "        url = 'https://techcabal.com'\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find recent articles\n",
        "            post_links = soup.find_all('h2', class_='post-title')\n",
        "\n",
        "            for post in post_links[:15]:\n",
        "                link_tag = post.find('a')\n",
        "                if link_tag:\n",
        "                    title = link_tag.get_text(strip=True)\n",
        "                    link = link_tag['href']\n",
        "\n",
        "                    if any(kw in title.lower() for kw in ['ai', 'artificial', 'tech', 'startup']):\n",
        "                        articles.append({\n",
        "                            'title': title,\n",
        "                            'url': link,\n",
        "                            'source': 'TechCabal',\n",
        "                            'scraped_at': datetime.now().isoformat()\n",
        "                        })\n",
        "\n",
        "            logger.info(f\"TechCabal: Scraped {len(articles)} articles\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"TechCabal scraping error: {e}\")\n",
        "\n",
        "        return articles\n",
        "\n",
        "    def run(self) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Run all scrapers\"\"\"\n",
        "\n",
        "        companies = self.scrape_disrupt_africa()\n",
        "        articles = self.scrape_techcabal()\n",
        "\n",
        "        return {\n",
        "            'companies': pd.DataFrame(companies),\n",
        "            'articles': pd.DataFrame(articles)\n",
        "        }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzVPgTCEqllB"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MASTER ORCHESTRATOR\n",
        "# ============================================================================\n",
        "\n",
        "class AfricaAIMasterPipeline:\n",
        "    \"\"\"Orchestrate all data collection pipelines\"\"\"\n",
        "\n",
        "    def __init__(self, config: APIConfig):\n",
        "        self.config = config\n",
        "        self.results = {}\n",
        "\n",
        "    def run_all_pipelines(self, countries: List[str], sectors: List[str]):\n",
        "        \"\"\"Execute all data collection pipelines\"\"\"\n",
        "\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"STARTING AFRICA AI INTELLIGENCE PLATFORM\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "        # # Pipeline 1: Crunchbase Companies\n",
        "        # if self.config.CRUNCHBASE_API_KEY != 'YOUR_KEY_HERE':\n",
        "        #     logger.info(\"\\n[1/5] Running Crunchbase Pipeline...\")\n",
        "        #     crunchbase = CrunchbaseAIPipeline(self.config.CRUNCHBASE_API_KEY)\n",
        "        #     self.results['crunchbase_companies'] = crunchbase.run(countries)\n",
        "        #     if 'crunchbase_companies' in self.results and not self.results['crunchbase_companies'].empty:\n",
        "        #         logger.info(f\"‚úÖ Crunchbase Pipeline finished: Collected {len(self.results['crunchbase_companies'])} companies.\")\n",
        "        #     else:\n",
        "        #         logger.warning(\"üö´ Crunchbase Pipeline finished: No companies collected or an issue occurred.\")\n",
        "        # else:\n",
        "        #     logger.warning(\"[1/5] Skipping Crunchbase - No API key\")\n",
        "\n",
        "        # Pipeline 2: GitHub Projects\n",
        "        if self.config.GITHUB_TOKEN != 'YOUR_KEY_HERE':\n",
        "            logger.info(\"\\n[2/5] Running GitHub Pipeline...\")\n",
        "            github = GitHubAIPipeline(self.config.GITHUB_TOKEN)\n",
        "            self.results['github_projects'] = github.run(countries, sectors)\n",
        "            if 'github_projects' in self.results and not self.results['github_projects'].empty:\n",
        "                logger.info(f\"‚úÖ GitHub Pipeline finished: Collected {len(self.results['github_projects'])} projects.\")\n",
        "            else:\n",
        "                logger.warning(\"üö´ GitHub Pipeline finished: No projects collected or an issue occurred.\")\n",
        "        else:\n",
        "            logger.warning(\"[2/5] Skipping GitHub - No API token\")\n",
        "\n",
        "        # Pipeline 3: News Articles\n",
        "        if self.config.NEWSAPI_KEY != 'YOUR_KEY_HERE':\n",
        "            logger.info(\"\\n[3/5] Running NewsAPI Pipeline...\")\n",
        "            news = NewsAIPipeline(self.config.NEWSAPI_KEY)\n",
        "            self.results['news_articles'] = news.run(countries)\n",
        "            if 'news_articles' in self.results and not self.results['news_articles'].empty:\n",
        "                logger.info(f\"‚úÖ NewsAPI Pipeline finished: Collected {len(self.results['news_articles'])} articles.\")\n",
        "            else:\n",
        "                logger.warning(\"üö´ NewsAPI Pipeline finished: No articles collected or an issue occurred.\")\n",
        "        else:\n",
        "            logger.warning(\"[3/5] Skipping NewsAPI - No API key\")\n",
        "\n",
        "        # Pipeline 4: Government Policies\n",
        "        if self.config.SERPAPI_KEY != 'YOUR_KEY_HERE':\n",
        "            logger.info(\"\\n[4/5] Running SerpAPI Government Pipeline...\")\n",
        "            serp = SerpAPIGovPipeline(self.config.SERPAPI_KEY)\n",
        "            self.results['government_policies'] = serp.run(countries)\n",
        "            if 'government_policies' in self.results and not self.results['government_policies'].empty:\n",
        "                logger.info(f\"‚úÖ SerpAPI Pipeline finished: Collected {len(self.results['government_policies'])} policy documents.\")\n",
        "            else:\n",
        "                logger.warning(\"üö´ SerpAPI Pipeline finished: No policy documents collected or an issue occurred.\")\n",
        "        else:\n",
        "            logger.warning(\"[4/5] Skipping SerpAPI - No API key\")\n",
        "\n",
        "        # Pipeline 5: African Tech Sites (No API needed)\n",
        "        logger.info(\"\\n[5/5] Running African Tech Scraper...\")\n",
        "        tech_scraper = AfricanTechScraper()\n",
        "        scraper_results = tech_scraper.run()\n",
        "        self.results.update(scraper_results)\n",
        "        if 'companies' in self.results and not self.results['companies'].empty:\n",
        "            logger.info(f\"‚úÖ African Tech Scraper (Companies) finished: Collected {len(self.results['companies'])} companies.\")\n",
        "        else:\n",
        "            logger.warning(\"üö´ African Tech Scraper (Companies) finished: No companies collected or an issue occurred.\")\n",
        "        if 'articles' in self.results and not self.results['articles'].empty:\n",
        "            logger.info(f\"‚úÖ African Tech Scraper (Articles) finished: Collected {len(self.results['articles'])} articles.\")\n",
        "        else:\n",
        "            logger.warning(\"üö´ African Tech Scraper (Articles) finished: No articles collected or an issue occurred.\")\n",
        "\n",
        "        logger.info(\"\\n\" + \"=\"*80)\n",
        "        logger.info(\"ALL PIPELINES COMPLETED\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def generate_summary(self):\n",
        "        \"\"\"Generate summary statistics\"\"\"\n",
        "        summary = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_datasets': len(self.results),\n",
        "            'datasets': {}\n",
        "        }\n",
        "\n",
        "        for name, df in self.results.items():\n",
        "            if isinstance(df, pd.DataFrame):\n",
        "                summary['datasets'][name] = {\n",
        "                    'records': len(df),\n",
        "                    'columns': list(df.columns)\n",
        "                }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def export_results(self, output_dir: str = './'):\n",
        "        \"\"\"Export all results to files\"\"\"\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Check if there's any non-empty DataFrame to export\n",
        "        has_data = any(isinstance(df, pd.DataFrame) and not df.empty for df in self.results.values())\n",
        "\n",
        "        if not has_data:\n",
        "            logger.warning(\"No data collected to export. Skipping file export.\")\n",
        "            return\n",
        "\n",
        "        # Export individual CSVs\n",
        "        for name, df in self.results.items():\n",
        "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                filename = f\"{output_dir}africa_ai_{name}_{timestamp}.csv\"\n",
        "                df.to_csv(filename, index=False)\n",
        "                logger.info(f\"‚úÖ Exported: {filename} ({len(df)} records)\")\n",
        "\n",
        "        # Export combined Excel\n",
        "        excel_file = f\"{output_dir}africa_ai_complete_{timestamp}.xlsx\"\n",
        "        try:\n",
        "            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
        "                for name, df in self.results.items():\n",
        "                    if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                        df.to_excel(writer, sheet_name=name[:31], index=False)\n",
        "            logger.info(f\"‚úÖ Combined Excel: {excel_file}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error exporting combined Excel file: {e}\")\n",
        "\n",
        "        # Export summary\n",
        "        summary = self.generate_summary()\n",
        "        summary_file = f\"{output_dir}pipeline_summary_{timestamp}.json\"\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        logger.info(f\"‚úÖ Summary: {summary_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09qsn6L_qw-k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Configuration\n",
        "    COUNTRIES = [\n",
        "        'Algeria', 'Angola', 'Benin', 'Botswana', 'Burkina Faso', 'Burundi',\n",
        "        'Cabo Verde', 'Cameroon', 'Central African Republic', 'Chad', 'Comoros',\n",
        "        'Congo, Dem. Rep.', 'Congo, Rep.', 'Cote dIvoire', 'Djibouti', 'Egypt',\n",
        "        'Equatorial Guinea', 'Eritrea', 'Eswatini', 'Ethiopia', 'Gabon', 'Gambia',\n",
        "        'Ghana', 'Guinea', 'Guinea-Bissau', 'Kenya', 'Lesotho', 'Liberia', 'Libya',\n",
        "        'Madagascar', 'Malawi', 'Mali', 'Mauritania', 'Mauritius', 'Morocco',\n",
        "        'Mozambique', 'Namibia', 'Niger', 'Nigeria', 'Rwanda', 'Sao Tome and Principe',\n",
        "        'Senegal', 'Seychelles', 'Sierra Leone', 'Somalia', 'South Africa', 'South Sudan',\n",
        "        'Sudan', 'Tanzania', 'Togo', 'Tunisia', 'Uganda', 'Zambia', 'Zimbabwe'\n",
        "    ]\n",
        "\n",
        "    SECTORS = [\n",
        "        'Agriculture', 'Healthcare', 'Fintech', 'Education',\n",
        "        'Logistics', 'Energy', 'Governance', 'Telecommunications', 'Mining',\n",
        "        'Tourism', 'Manufacturing', 'Retail', 'Media', 'Real Estate',\n",
        "        'Transportation', 'Financial Services', 'Public Sector', 'Utilities',\n",
        "        'Environment', 'Smart Cities', 'Security', 'Creative Industry','Insurance'\n",
        "    ]\n",
        "\n",
        "    # Initialize pipeline\n",
        "    config = APIConfig()\n",
        "    pipeline = AfricaAIMasterPipeline(config)\n",
        "\n",
        "    # Run all pipelines\n",
        "    results = pipeline.run_all_pipelines(COUNTRIES, SECTORS)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESULTS PREVIEW\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    for name, df in results.items():\n",
        "        if isinstance(df, pd.DataFrame):\n",
        "            print(f\"\\nüìä {name.upper()}: {len(df)} records\")\n",
        "            print(\"-\" * 60)\n",
        "            if not df.empty:\n",
        "                print(df.head(3))\n",
        "\n",
        "    # Export everything\n",
        "    pipeline.export_results()\n",
        "\n",
        "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
        "    print(\"\\nüìù NEXT STEPS:\")\n",
        "    print(\"1. Add your API keys to APIConfig class\")\n",
        "    print(\"2. Run the pipelines\")\n",
        "    print(\"3. Check exported CSV and Excel files\")\n",
        "    print(\"4. Analyze the data for insights\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
